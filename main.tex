\documentclass{article} % For LaTeX2e
\usepackage{iclr2020_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{breqn}
\usepackage{booktabs}
\usepackage{siunitx} % used in SHREC table
\usepackage{bbm}        % for mathbbm
\usepackage{graphicx} \graphicspath{{figures/}}
%\usepackage{wrapfig}
%\usepackage{floatflt}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{prop}{Proposition}
\newtheorem*{remark}{Remark}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

%\title{Which spherical CNN should you use?\\ DeepSphere V2}
\title{DeepSphere: a graph-based spherical CNN}
%\title{DeepSphere: an efficient graph-based spherical CNN}
%\title{DeepSphere, an efficient (& equivariant) graph-based spherical CNN}

\author{Michaël Defferrard, Martino Milani \& Frédérick Gusset \\
Institute of Electrical Engineering, EPFL, Lausanne, Switzerland \\
\texttt{\{michael.defferrard,martino.milani,frederick.gusset\}@epfl.ch}
\AND
Nathanaël Perraudin \\
Swiss Data Science Center (SDSC), Zurich, Switzerland \\
\texttt{nathanael.perraudin@sdsc.ethz.ch}
}

\renewcommand{\b}[1]{{\bm{#1}}}   % bold symbol

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\todo}[1]{{\color[rgb]{.6,.1,.6}{#1}}}
\newcommand{\nati}[1]{{\color[rgb]{.3,.5,.9}{#1}}}
\newcommand{\mart}[1]{{\color[rgb]{.9,.5,.3}{#1}}}


\renewcommand{\figref}[1]{figure~\ref{fig:#1}}
\renewcommand{\Figref}[1]{Figure~\ref{fig:#1}}
\newcommand{\tabref}[1]{table~\ref{tab:#1}}
\newcommand{\Tabref}[1]{Table~\ref{tab:#1}}
\renewcommand{\secref}[1]{section~\ref{sec:#1}}
\renewcommand{\Secref}[1]{Section~\ref{sec:#1}}
%\newcommand{\secref}[1]{\S\ref{sec:#1}}
\newcommand{\eqnref}[1]{equation~(\ref{eqn:#1})}
\newcommand{\Eqnref}[1]{Equation~(\ref{eqn:#1})}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\todo{
\begin{itemize}
	\item shall we really name it? Or simply reference the general idea of a graph-based spherical CNN.
	\item adjacency matrix: $A$ instead of $W$?
	\item anonymize DeepSphere citation
% 	\item climate spherical figure: inputs with TC and AR labels superimposed [Frédérick]
	\item citet for inline, citep for parenthesis
	\item More details on related work? Not sure we have the space. If yes, detailed comparison with related work after method.
	\item PSD of climate data: how does it compare to cosmo and 3D objects?
	\item align the spheres in figure 1
	\item mention that \citep{esteves2018sphericalcnn} and \citep{cohen2018sphericalcnn} computed equivariance error
	\item check footnotes
	\item climate data on spectrum plot
	\item result tables: only what supports the main message in the text, everything we have in the appendix
\end{itemize}
}
\newpage

\maketitle

\begin{abstract}

story: there are tradeoffs when designing a spherical CNN -> DS strikes a controllable balance between efficiency / cost and exactitude / equivariance
% DSv2 has a clear tradeoff between equivariance and computational cost

* method development: tool that solve a need (balance btw the desiderata: equiv vs cost)\\
* research question: anisotropy (highlight from the start or leave it as a dangling question in the end)\\

contributions:\\
* theory $\Rightarrow$ convergence (better graph)\\
* experiments on relevant problems (not spherical MNIST) $\Rightarrow$ check the desiderata\\
  * surprising: anisotropy doesn't seem useful\\

Experiments on xxx show
Comparison with previous work on shape classification and climate event detection suggests that anisotropic filters are an unnecessary price to pay.
% memory, computations, implementation complexity

\end{abstract}

\section{Introduction [2 pages]}

Spherical data is found in many applications.
% spherical data f: S^2 -> R, projection from R^3
Planetary data (such as meteorological or geological measurements) and brain activity are example of intrinsically spherical data.
The observation of the universe, LIDAR scans, and the digitalization of 3D objects are examples of projections due to observation.
See \figref{examples}.
In many applications, labels or variables are to be inferred from spherical data.
Examples are the inference of cosmological parameters from the distribution of mass in the universe \citep{perraudin2019deepspherecosmo}, the segmentation of omnidirectional images \citep{khasanova2017sphericalcnn}, and the segmentation of cyclones from Earth observation \citep{mudigonda2017climateevents}.

\begin{figure}[h]
	\includegraphics[height=0.25\linewidth]{example_cosmo_cmb}
	\hfill
	\includegraphics[height=0.25\linewidth]{example_brain_meg}
% 	\hfill
% 	\includegraphics[height=0.25\linewidth]{example_climate_TMQ}
	\hfill
	\includegraphics[height=0.25\linewidth]{example_ghcn_daily_tmax}
	\hfill
	\includegraphics[height=0.25\linewidth]{example_ghcn_graph}
	\caption{
		Examples of intrinsically spherical data:
		(left) the cosmic microwave background (CMB) temperature from \citet{planck2015overview},
		(middle) brain activity recorded through magnetoencephalography (MEG),\protect\footnotemark
		(right) daily maximum temperature from the Global Historical Climatology Network (GHCN).\protect\footnotemark
		A rigid full-sphere pixelization is not ideal: the Milky Way's galactic plane masks observations, brain activity is only measured on the scalp, and the position of weather stations is arbitrary and changes over time.
		Graphs can faithfully and efficiently represent sampled spherical data by placing vertices where data has been measured.
		%\todo{something from climate, weather?}
		%\todo{compare ideal full-spheres (like SHREC-17 projection from Esteves, 360 from Coors) to real-world measurements?}
		%\todo{horizontal colorbar for brain}
	}
	\label{fig:examples}
\end{figure}
\footnotetext[1]{\scriptsize\url{https://www.ncdc.noaa.gov/ghcn-daily-description}}
\footnotetext[2]{\scriptsize\url{https://martinos.org/mne/stable/auto_tutorials/plot_visualize_evoked.html}}

% balance of ??
% Exploit the geometrical properties of the domain, symmetries.
% desiderata or tradeoff
% We identified the following desiderata for a spherical NN.
%Those take different tradeoffs among the following desiderata.
%\begin{enumerate}
%	\item \textbf{Respect the geometry.}  % geometry, domain
%		By respecting the domain, a spherical NN should respect the symmetries of the sphere, i.e., be equivariant to rotation, and not deform the sphere, e.g., by a projection on the icosahedron.
%	\item \textbf{Generality.} % powerful
%		How general the NN is as a function approximator.
%		For example, isotropic filters are less general than anisotropic filters.
%	\item \textbf{Scalability.}
%		How a spherical CNN scales w.r.t.\ the number of pixels in terms of computational cost and memory usage.
%		% memory: feature maps and parameters
%	\item \textbf{Flexibility.}
%		Whether the NN is constrained to work on specific samplings, part of the sphere (useful when data is masked), and irregular samplings.
%	\item \textbf{Simplicity.}
%		Simplicity of its derivation and implementation.
%\end{enumerate}

% keep the story simple and focus on the tradeoff between cost (time / computational and storage / memory) and exactitude
As neural networks (NNs) have proved to be great tools for inference, variants have been developed to handle spherical data.
Exploiting the locally Euclidean property of the sphere, early attempts used standard 2D convolutions on a grid discretization of the sphere \citep{boomsma2017sphericalcnn, su2017sphericalcnn, coors2018sphericalcnn}.
While simple and efficient, those convolutions are not equivariant to rotations.
On the other side of this tradeoff, \citet{cohen2018sphericalcnn} and \citet{esteves2018sphericalcnn} proposed to perform proper spherical convolutions through the spherical harmonic transform.
While equivariant to rotations, those convolutions are expensive.

% On the one side, we have Cohen which is computationally very expensive but perfectly equivariant most general, on the other we have cube sphere which computationl very good but not equivariant at all. Other methods are tradedoff between these two. What is a good tradeoff? We think that DeepSphere is.

As the lack of equivariance penalizes performance (\secref{exp:cosmo}) and expensive convolutions prohibit their application to some real-world problems, methods standing between those two extremes are desired.
\citet{cohen2019gauge} proposed to reduce costs by limiting the size of the representation of the symmetry group.
% (from the continuous rotation group SO(3) to a discrete rotation group)
% the dimensionality of feature maps is equal to the size of the irreducible representation of the symmetry group
%As a platonic solid, it \todo{exhibits discrete rotations}.
% the representation of its symmetry group is much smaller than SO(3)
\todo{not all rotations, subset of SO(3)?}
The projection of the data from the sphere to the icosahedron however introduces distortions that might hinder performance (see the experiments in \secref{exp:climate}).
\todo{Where does Jiang fit? Need a global coordinate system (ok for planets, not projections like cosmo)}

Another approach represents the discretized sphere as a graph connecting pixels according to the distance between them \citep{khasanova2017sphericalcnn, perraudin2019deepspherecosmo}.
While Laplacian-based graph convolutions are more efficient than spherical convolutions, they are not exactly equivariant \citep{defferrard2019deepsphereequiv}.
In this work, we argue that graph-based spherical CNNs strike an interesting balance, with a controllable tradeoff between cost and equivariance.
Experiments on multiple problems of practical interest show the competitiveness and flexibility of this approach.
% Furthermore, experiments show that, perhaps surprisingly and contrarily to previously published results \citep{cohen2019gauge} \todo{[Bronstein?]}, anisotropic filters might be an unnecessary price to pay.
% This paper focuses on improving the rotation equivariance of \citet{perraudin2019deepspherecosmo}, a scalable graph-based method designed for cosmological applications.

\section{Method [1 pages]}

The gist of the method is to define the spherical convolution using a graph, and the down-sampling operation using a hierarchical pixelisation of the sphere.

\todo{
	* discuss discretizations (equiangular, HEALPix) -> more uniform is better \\
	* no uniform sampling: tradeoffs have to be made \\
	* CNN vs FCN discussion (GAP for invariance) \\
}

* HEALPix: improvement from DeepSphere v1 \cite{perraudin2019deepspherecosmo} \\
* equiangular: Renata \& Pascal \\

% \cite{khasanova2017sphericalcnn} designed a discrete Laplacian that is explicitly intended to work on the sphere with the equiangular sampling. They consider the set $\mathcal G$ of all the possible graphs where each node is connected only to four of its nearest neighbours (North, Sud, West, East) and propose a weighting scheme $w_{ij}$ that minimizes the difference in the response to the polynomial spectral filter $\mathcal F = \mathbf L$ evaluated on images of the same object seen at different latitudes. In other words, they solve the minimization problem
% \begin{equation} \label{eq:minimization frossard}
% 	\min_{W\in\mathcal G} \left|\mathcal{F}\left(\mathbf{y}\left(v_{ e}\right)\right)-\mathcal{F}\left(\mathbf{y}\left(v_{ i}\right)\right)\right|
% \end{equation}
% for the adjacency matrix $W$, where $\mathbf y(v_i)$ is the image of the object on the sphere centered on the vertex $v_i$, and $\mathcal F (\mathbf y(v_e))$ is the response of the filter at the vertex $v_e$ that lies at the same longitude of the vertex $v_i$ but on the equator. In their work they prove that the optimal weights solving the minimization problem (\ref{eq:minimization frossard}) are given by weights $w_{ij}$ inversely proportional to the Euclidean distance between vertices:
% \begin{equation} \label{eq:frossard weights}
% 	w_{ij} = \frac{1}{\norm{x_i-x_j}}
% \end{equation}

\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{figure_architecture_v3}
	\caption{Example architecture from \cite{perraudin2019deepspherecosmo}.
		Global tasks need a spatial summarization: the FCN variant is rotation invariant (and accepts inputs of varying sizes), while the CNN variant is not.
		Dense tasks (when the output lives on the sphere, like segmentation) are rotation equivariant.
	}
	\label{fig:architecture}
\end{figure}

% \section{Notation}
% Throughout this contribution, we denote with bold font all vectors and matrices such as $\b{L}$ the graph Laplacian and $\b{f}$ the vector of the sampled function $f$.
% \nati{Right now, it is not True}
% % , a weighted undirected graph $G(V, E, \mathbf W)$,
% \nati{I do not think we should have a notation section, but should somehow write this somewhere...}

\section{Is graph convolution equivariant to rotation? [2 pages]} \label{sec:equivariance}

\subsection{Problem formulation}
% The plan for this section
% 0.1 Explain the sampling problem
As the graph convolution acts on a sampled version of the function $\b{f}$ and the rotation on its continuous counterpart $f$, a sampling operator is needed.
Given a sampling scheme $V=\{v_i\in\mathbb S^2, i=0, \dots, n\}$ of the sphere and a function $f : \mathbb S^2 \supset F_V \to \mathbb R$, the sampling operator $T_V: L^2(\mathbb S^2) \supset F\to \mathbb R^n,\  T_V(f) = \b{f}$ is defined as $\b{f}:\ f_i=f(v_i)$, where $F_V$ suitable subspace of $L^2(\mathbb S^2)$ such that $T_V$ is invertible, i.e., we can unambiguously reconstruct the function $f\in F_V$ from its sampled values $\b{f}$.
The existence of such subspace depends on the sampling scheme $V$, and its characterization is a common problem in signal processing (\cite{driscoll1994Fouriersphere}).
For most sampling schemes, it is not known if $F_V$ exist and hence if $T_V$ is invertible. A special case is the \textit{equiangular sampling} scheme where a sampling theorem holds, and thus we have a closed form of $T_V^{-1}$ is known. % \cite{driscoll1994Fouriersphere}
For sampling schemes where no such sampling formula is available, we leverage the discrete SHT to reconstruct $f$ from $\bf{f}$, thus approximating $T_V^{-1}$.
For all theoretical consideration, we assume that $F_V$ exists and $f \in F_V$.

% 0.2 Explain why the Laplacian is so important...
By definition, our (spherical) graph convolution is rotation equivariant, if and only if, it commutes with the rotation operator defined as $R(g), g\in SO(3)$: $R(g) f(x) = f\left(g^{-1} x \right)$.
In the context of this work, graph convolution is performed by recursive applications of the graph Laplacian, i.e: $h(\b{L}) \b{f} = \left(\sum_{i=0}^K \alpha_i \b{L}^i\right) \b{f}$\nati{Put this equation somewhere else?}.  Hence, if $\b{L}$ commutes with the rotation operator, then, by recursion, it will also commute with the convolution $h(\b{L})$.
As a result, to simplify our analysis, we concentrate on the graph Laplacian and $h(\b{L})$ is rotational invariant if and only if
% 0.3 Formulate the problem
\begin{equation} \label{eq:equivariance}
	\b{R}_V(g) \b{L} \b{f} = \b{L} \b{R}_V(g) \b{f} \hspace{1cm} \forall f\in F_V \text{ and } \forall g\in SO(3),
\end{equation}
where $\b{R}_V(g) = T_V R(g) T_V^{-1}$. For practical a evaluation of the equivariance, we define the \textit{normalized equivariance error} for a signal $\b{f}$, and a  rotation $g$ as:
\begin{equation} \label{eq:equivariance error}
	E_{\b{L}}(\b{f}, g) = \left(\frac{ \norm {\b{R}_V(g) \b{L} \b{f} - \b{L} \b{R}_V(g) \b{f}} }{\norm {\b{f}}}\right)^2,
\end{equation}
More generally for a class of signal $f \in C \subset F_V$, the \textit{mean equivariance error}
\begin{equation} \label{eq:mean equivariance error}
	\overline E_{\b{L}, C} = \mathbb E_{\b{f}\in C, g\in SO(3)} \ E_{\b{L}}(\b{f}, g).
\end{equation}
\mart{Talk about the probability measure used on C, SO(3)?}
In practice, to analyze the equivariance error frequency,  the set $C$ we used is composed of functions $f$ made of spherical harmonic of a single degree $\ell$.
The expected value is obtained by averaging over a finite number of random functions and random rotations.

\subsection{Finding the optimal set of weights}
% !) summary the work of Khasanova -> No! to be put in section 2) Method
\cite{khasanova2017sphericalcnn} designed a Laplacian $\b{L}$ explicitly intended to work with the equiangular sampling.
They consider the set $\mathcal{G}$ of all the possible graphs where each node is connected only to four of its nearest neighbours (North, Sud, West, East) and propose a weighting scheme $w_{ij}$ that minimizes \eqref{eq:equivariance error} for specific rotations on the longitude latitude axis.
Their approach leads to weights $w_{ij}$ inversely proportional to the Euclidean distance between vertices:
\begin{equation} \label{eq:frossard weights}
	w_{ij} = \frac{1}{\norm{x_i-x_j}}
\end{equation}

% 2) explain our approach
We chose a different approach and take inspiration from \cite{belkin2005towards}, which proves that for a random \emph{uniform sampling} scheme the graph Laplacian converges toward the Laplace-Beltrami operator $\Delta_{\mathbb{S}^2}$ as the number of sampling points goes to infinity.
This convergence result is important as a) $\Delta_{\mathbb{S}^2}$ commutes with rotation ($\Delta_{\mathbb{S}^2}R(g) = R(g)\Delta_{\mathbb{S}^2}$) and b) is diagonalized by the spherical harmonics.
In this case the weighting scheme is full (every node is connected to every node) and is based on the exponential kernel:
\begin{equation} \label{eq:belkin_weights}
w_{ij} = e^{-\frac{1}{4t} \norm{x_i-x_j}^2}
\end{equation}
In practice most of the weights are very close to $0$ and hence, to optimize for computational efficiency, we limit ourselves to only the $k$ nearest neighbors. Given $k$, we find the optimal $t$, a grid search is performed using \eqref{eq:mean equivariance error}.


\subsection{Analysis of the proposed weighting scheme}
We analyse our proposed weighting scheme both theoretically and empirically.

\paragraph{Theoretical convergence.}
We extend the work of Belkin and Nyiogi to a fixed sampling.
Given a sampling $V = \{x_0, \dots, x_{n-1}\}$, define $\sigma_i$ to be the patch of the surface of the sphere corresponding to $x_i$, define $A_i$ to be its corresponding area and $d_i$ to be the radius of the smallest ball in $\mathbb R^3$ containing $\sigma_i$. Define $d^{(n)} := \max_{i=0, \dots, n}d_i$ and $A^{(n)}=\max_{i=0, \dots, n}A_i$.

\begin{theorem}
	For a sampling $V = \{x_i\in\mathbb S^2\}_{i=0}^{n-1}$ of the sphere that is equi-area and such that $d^{(n)} \leq \frac{C}{\sqrt{n}}$, for all $f: \mathbb S^2 \rightarrow \mathbb R$ Lipschitz with respect to the Euclidean distance in $\mathbb R^3$, for all $y\in\mathbb S^2$, there exists a sequence $t_n = n^\beta$ such that the rescaled Heat Kernel Graph Laplacian $\frac{|\mathbb S^2|}{4\pi t_n}\b {L}^t_n$ converges pointwise to the Laplace Beltrami operator on the sphere $\Delta{\mathbb S^2}$  for $n\to\infty$:
	$$ \lim_{n\to\infty} \frac{|\mathbb{S}^2|}{4\pi t_n} \b{L}_n^{t_n} T_V f(x_i) =  \Delta_{\mathbb{S}^2}f(x_i)\quad \forall i=0, ..., n-1.$$
	\label{theo:pointwise convergence for a regular sampling}
\end{theorem}
This theorem implies immediately that $\b{L}$ converges pointwisely toward an operator ($\Delta_{\mathbb S^2}$) commuting with rotation and hence is a strong hint for equivariance.
Importantly, the proof of Theorem \ref{theo:pointwise convergence for a regular sampling} in Appendix \ref{...} inspires us in the construction of our graph Laplacian. It gives us a strong hint how the parameter $t$ needs to be adjusted with respect to the number of points $n$.

It is important to keep in mind the limits of Theorem \ref{theo:pointwise convergence for a regular sampling}.
First, it is \emph{not} a proof of equivariance of the graph convolution for a specific sampling. To construct such a proof, a stronger convergence result is needed, for example uniform convergence (instead of pointwise).
Unfortunately, the graph laplacian does \emph{not} converges uniformly in general \cite{belkin2007convergence}. This can probably be overcome by considering a specific class of functions, such as bandlimited function.
Second, while do not have a proof for it, we strongly believe that the HEALPix sampling does satisfy the hypothesis $d^{(n)}\leq \frac{C}{\sqrt{n}}$ of Theorem \ref{theo:pointwise convergence for a regular sampling}. This is discussed in Appendix~\ref{...}


% 3) Show experimental results
\paragraph{Empirical convergence.}

% 3.0) explain how we obtained the figures
We calculated numerically the equivariance error (\ref{eq:mean equivariance error}) by generating $N$ monochromatic signals and $M$ random rotations for each signal. The colors that were used were chosen in the range $(0,\  3N_{side}-1)$ to allow for an almost perfect synthesis of the rotated signals, and thus an almost perfect implementation (up to numerical errors) of the operator $\b{R}_V$ (\cite{healpix_primer})

% 3.1) explain what the figures tell us
From 20\% error on V1, we got to less than 2\% error on V2. The number of neighbors used for V2 is $50$. More neighbors implies naturally more precision, plus a better kernel width obtained by "minimizing" the equivariance error. \mart{How to explain how we found the kernel width? Micha\"el I need your help here!!} Literally, minimizing the kernel width would lead to a completely disconnected graph. What we did was finding a heuristic law (fix neighbors, set the kernel width $\hat t$ by imposing the minimum weight to be equal to $0.1$) and then minimizing the equivariance error in the range $(\hat t / 10, \hat t\cdot 10) $. We got better that Khasanova-Frossard \mart{(check the code)} that however uses only 4 neighbors. However, it is built to explicitely minimize the equivariance error, so we expect it to be a good benchmark against which to test our construction.
\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{DeepSphereV1}
	\caption{Shall we put it here or in Appendix?
	}
	\label{fig:deepsphereV1}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{DeepSphereV2_50neighbors.pdf}
	\caption{}
	\label{fig:deepsphereV2}
\end{figure}
\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{khasanova_frossard.pdf}
	\caption{}
	\label{fig:khasanova_frossard}
\end{figure}


\section{Experiments [2 pages]}

\todo{
Show: \\
* meet the desiderata \\
* DeepSphere V1 and V2 are equivalent in practice, wait on SHREC and cosmo exp \\
* anisotropy doesn't help \\
}

\subsection{3D objects recognition} \label{sec:exp:objects}

\begin{figure}
%\begin{floatingfigure}[r]{0.5\linewidth}
%\begin{wrapfigure}{r}{0.5\linewidth}
	\begin{minipage}[t]{0.57\linewidth}
		\centering
		\includegraphics[height=7em]{lamp_000018}
		\hfill
		\includegraphics[height=7em]{lamp_000018_sphere_nobar}
		\caption{Example of a 3D object represented as a spherical depth map.}
		\label{fig:depthmap}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.35\linewidth}
		\centering
		\includegraphics[height=7em]{spectrum}
		\caption{Power spectral densities.}
		\label{fig:spectrum}
	\end{minipage}
%\end{wrapfigure}
%\end{floatingfigure}
\end{figure}

The recognition of 3D shapes is a rotation invariant task: rotating an object doesn't change its nature.
While 3D shapes are usually represented as meshes or point clouds, representing them as spherical maps (\figref{depthmap}) naturally allows a rotation invariant treatment.
LIDAR actually acquires depth maps, which are a posteriori represented as point clouds.

The SHREC'17 shape retrieval contest \citep{shrec17} contains 51,300 randomly oriented 3D models from ShapeNet \citep{shapenet}, to be classified in 55 categories (tables, lamps, airplanes, etc.).
As in \citet{cohen2018sphericalcnn}, objects are represented by 6 spherical maps.
At each pixel, a ray is traced towards the center of the sphere.
The distance from the sphere to the object forms a depth map.
The $\cos$ / $\sin$ of the surface angle forms two normal maps.
The same is done for the object's convex hull.
The maps are discretized using either an equiangular grid \citep{driscoll1994Fouriersphere} with bandwidth $b = 64$ ($N_{pix} = 4 b^2 = 16,384$), or an HEALPix \citep{healpix} grid with $N_{side} = 32$ ($N_{pix} = 12 N_{side}^2 = 12,288$).

\todo{Graph (\#neighbors), network architecture (info in the appendix, set the notation in the method section) and hyper-parameters. Or leave in appendix.}

\begin{table}%[ht]
    \centering
	%\begin{tabular}{l cc r rr}
	\begin{tabular}{l cc r rS[table-format=2.0]}
		\toprule
		& \multicolumn{2}{c}{performance} & \multicolumn{1}{c}{size} & \multicolumn{2}{c}{speed} \\
		\cmidrule(lr){2-3} \cmidrule(lr){4-4} \cmidrule(lr){5-6}
		& F1 & mAP & params & \multicolumn{1}{c}{inference} & \multicolumn{1}{c}{training} \\
		\midrule
		% $SO(3)$ vs $S^2$ vs graph
		\citet{cohen2018sphericalcnn} ($b=128$) & 78.9 & 67.6 & 1400\,k & 38.0\,ms & 50\,h \\
		\citet{cohen2018sphericalcnn} (simplified,\protect\footnotemark $b=64$) & 78.6 & 66.5 & 400\,k & 12.0\,ms & 32\,h \\
		\citet{esteves2018sphericalcnn} ($b=64$) & 79.4 & 68.5 & 500\,k & 9.8\,ms & 3\,h \\
		DeepSphere (equiangular $b=64$) & 79.4 & 66.5 & 190\,k & 0.9\,ms & 50\,m \\
		DeepSphere (HEALPix $N_{side}=32$) & 80.7 & 68.6 & 190\,k & 0.9\,ms & 50\,m \\
		\bottomrule
	\end{tabular}
    \caption{
		Results on SHREC'17 (3D shapes): DeepSphere achieves similar performance at a much lower cost, suggesting that anisotropic filters are an unnecessary price to pay.
		% F1-score computed with sklearn, mAP from the official script of the competition.
		% inference speed = time for a single instance to do a single training pass
		% training speed = time for the neural net to train to peak performance
	}
    \label{tab:shrec17}
\end{table}
\footnotetext[5]{As implemented in \url{https://github.com/jonas-koehler/s2cnn}.}

Results are shown in \tabref{shrec17}.
DeepSphere achieves the same performance as \citet{cohen2018sphericalcnn} and \citet{esteves2018sphericalcnn} while being much more efficient, suggesting that anisotropic filters are an unnecessary price to pay.
% x times faster
As the information in those spherical maps resides in the low frequencies (\figref{spectrum}), reducing the equivariance error didn't translate into improved performance.
For the same reason, using the more uniform HEALPix discretization or lowering the resolution down to $N_{side} = 8$ didn't impact performance.
\todo{The same conclusions are drawn from ModelNet 40 (see appendix ??).}

% Limited by representation of data? yep, treat them as point clouds

\subsection{Cosmological model discrimination} \label{sec:exp:cosmo}

\todo{
* other sphercial CNNs cannot scale to 10M pixels (tested on 10k at most) \\
* redo the experiment with the optimal graph \\
* DSv2 better than v1 -> equivariance has practical implications \\
* clear cost-accuracy tradeoff with n_neighbors \\
* show the baselines? (at least the 2D CNN?) \\
}

% \begin{table}[]
%     \centering
%     \begin{tabular}{c|c}
%          &  \\
%          &
%     \end{tabular}
%     \caption{Caption}
%     \label{tab:cosmo_newgraph}
% \end{table}

\subsection{Climate event segmentation} \label{sec:exp:climate}

* small: better perf than gauge and Jiang. due to icosahedron distortion? to be confirmed\\
* cannot compare with Mudigonda (16 vs 1 input channel)\\
* (full: we scale (50B pixels, 20TB) $\Rightarrow$ lead to better perf?) \\

\begin{table}
	\centering
	\begin{tabular}{l l l}
	\toprule
	& accuracy & mAP \\
	\midrule
	\cite{jiang2019sphericalcnn} (paper) & 94.67 & - \\
	\cite{jiang2019sphericalcnn} (rerun) & 94.95 & 38.41 \\
	\cite{cohen2019gauge} (S2R) & 97.5 & 68.6 \\
	\cite{cohen2019gauge} (R2R) & 97.7 & 75.9 \\
	DeepSphere (weighted loss) & $97.8\pm 0.3$ & $77.15\pm 1.94$ \\
	DeepSphere (non-weighted loss) & $87.8\pm 0.5$ & $89.16\pm 1.37$ \\
	\bottomrule
	\end{tabular}
	\caption{
		Climate event segmentation results: mean accuracy and mean average precision (mAP). DeepSphere achieves state-of-the-art performance while using isotropic filters (S2S in \citet{cohen2019gauge}'s language), suggesting again that anisotropic filters are an unnecessary price to pay.
		Note that \citet{jiang2019sphericalcnn} and \citet{cohen2019gauge} use a weighted cross-entropy loss, which is good for the accuracy metric (as it corrects for class imbalance) but bad for the mAP metric.
	}
\end{table}

* icosahedron dataset from \cite{jiang2019sphericalcnn} \\
* Jiang rerun with a batch size of 64 instead of 256 due to memory limit.\\
* cross entropy loss between parenthesis\\
* 5 runs for each loss to obtain mean and std\\
* same architecture as gauge, more feature maps than Jiang\\
* perf with same arch as Jiang in appendix => not enough channels!\\
* even more feature maps give us one more point in perf\\
* cannot compare with Mudigonda: not the same input data\\
* why: icosahedron, can do anisotropy with isotropy + non-linearity + pooling

* trade off between AP and accuracy in the case of TC class, it seems.\\
* change base, influence?\\

* full dataset not enough memory to run a correct model

% jiang like correct: 99.81631631, 13.18655162, 52.62412867, 55.20899887;  0.3990812 , 0.80056251, 0.59982186; 4 ms; 2h40
% more feat: 99.67871329, 38.78607557, 66.37910064, 68.2812965: 0.99981102, 0.60714174, 0.83236147, 0.71975161; 12ms; 6h20

\subsection{GHCN} \label{sec:exp:ghcn}

* task is artificial, but it shows DeepSphere's flexibility\\
* structure (looking around) help for prediction\\
* equivariance to be verified in this case (next paper)\\

* non-regular sampling with different density over the globe (continent vs ocean)\\
* resulting graph is not rotation equivariant\\
* Two different tasks: a dense regression and a global regression\\

\paragraph*{dense regression - future temperature estimation}~\\
* Task is to find the temperature at day T, knowing the temperature of the 5 previous days.\\
* Goal is to change the order of the chebyshev filter to find the influence of the structure.\\
* order 0 is the same as time series, as the pixel have no information of its neighborhood.\\
* increasing order is good until threshold.\\
* The NN indeed predicts the correct temperature at day T, and not day T-1 (baseline) even though there is small difference.\\



\paragraph*{global regression - day in year estimation}~\\
* days represented by a sine to represent the periodic nature of the year.\\
* task too easy, temperature feature already periodic $\Rightarrow$ second experiment with only precipitation feature\\
* structure is still helping\\

\begin{table}
    \centering
    \begin{tabular}{c|ccc}
        order & future temp & day (temperature) & day (precipitations) \\ \hline
        0 & 0.896 & 0.881 & - 0.920\\
        4 & 0.919 & 0.969 & 0.597\\
    \end{tabular}
    \caption{R2 coefficient for different tasks on the GHCN dataset}
    \label{tab:GHCN_results}
\end{table}

\section{Conclusion [0.5 pages]}

This work showed that DeepSphere strikes an interesting balance between desiderata for a spherical CNN.
A single parameter, the number of neighbors a pixel is connected to in the graph, controls the tradeoff between \todo{performance and efficiency}.
% vague words
As computational cost and memory consumption scales linearly with the number of pixels, DeepSphere easily deals with spherical maps made of millions of pixels.
Such high resolutions are not necessary to represent thethe  depth maps of 3D objects, but are required to faithfully represent cosmological and climate data.
% That has to do with the frequency content of the signals (see fig xx)
% cannot do better
Also relevant in scientific applications is the flexibility offered by a graph representation to deal with partial coverage and non-uniform samplings.
% partial coverage / missing data
Finally, the implementation of Laplacian-based graph convolutions is straightforward.
The ubiquity of graph neural networks, pushing for a first-class support in Deep Learning frameworks, will make implementations even easier and more efficient.

A potential drawback of graph Laplacian-based approaches is that graph filters are isotropic, reducing the expressive power of the NN.
% in principle -> may in fact not
% Our experiments showed that DeepSphere nonetheless achieved state-of-the-art performance, on par with methods featuring anisotropic filters.
% didn't hurt perf in any experiment
% expressive power / generality / powerful / function approximator
%Why anisotropic filters?
Experiments from \citet{cohen2019gauge} suggest that more general convolutions achieve better performance.
Our climate experiments (\secref{exp:climate}) however show that DeepSphere achieves similar performance than their most general convolution.
Works on shape alignment indicates that anisotropic filters achieve better performance \todo{[Bronstein]}.
Possible explanations for this discrepancy are that NNs somehow compensate for the lack of anisotropic filters, or that some tasks can be solved with isotropic filters.
The distortions induced by the icosahedral projection in \citep{cohen2019gauge} or the leakage of curvature information in \todo{[Bronstein]} might also be explanations.
Further research is needed to determine if anisotropic filters are an unnecessary price to pay or if they are needed for some applications.
% As was recently shown for graph NNs used for graph classification, most general might not be the best, when the ultimate goal is generalization [Andreas paper? + previous by xx]

Developing graph convolutions on irregular samplings that still respect the geometry of the sphere is another research direction of importance.
% known underlying manifold
Practitioners currently interpolate their measurements (coming from arbitrarily positioned weather stations, satellites or telescopes) to regular samplings.
This practice either results in a waste of precision or computational and storage resources.
Our ultimate goal is for practitioners to be able to work directly on their measurement, however distributed.

% beyond the sphere => any manifold
% what are other manifolds of interest?
% done by monet & co on shapes

% beyond scalar fields => simplicial complexes

\newpage
\subsubsection*{Author Contributions}
Left blank for anonymity reason
% If you'd like to, you may include  a section for author contributions as is done
% in many journals. This is optional and at the discretion of the authors.

\subsubsection*{Acknowledgments}
Left blank for anonymity reason

%We thank Taco Cohen for his inputs on the intriguing results of our comparisons with their gauge CNN \citep{}.

\bibliography{references}
\bibliographystyle{iclr2020_conference}

\newpage
{\LARGE \sc {Supplementary Material}}
\appendix

\section{Proof of theorem}
Given a sampling $\{x_i\in\mathcal M\}_{i=0}^{n-1}$ of a close, compact and infinitely differentiable manifold $\mathcal M$, a smooth function $f:\mathcal M \rightarrow \mathbb R$, and defined the vector $\b f$ of samples of $f$ as follows: $\b f\in \mathbb R^n,\ \b f_i = f(x_i)$, it is known that  (\cite{belkin2005towards}) to prove the convergence of $\b{L}_n^t \b f _i$ to $\Delta_{\mathbb S^2}f(x_i)$ it is sufficient to prove the convergence of its continuous counterpart $L_n^tf(x)$ to $\Delta_{\mathbb S^2}f(x)$
where $L_n^t$ is defined as follows:
\begin{definition}{}\\
	\label{def:Heat Kernel Graph Laplacian operator}
	\text{Given a sampling $\{x_i\in\mathcal M\}_{i=0}^{n-1}$ of the manifold we define the \textbf{operator} }$L_n^t$ such that
	$$L_n^tf(y) := \frac{1}{n}\left[ \sum_{i=0}^{n-1} \exp \left\{ {-\frac{||x_i-y||^2}{4t}}\right\}\left(f(y)-f(x_i)\right)\right]$$
\end{definition}
Thus, it will be sufficient to prove that 
\begin{equation}\label{eq:continuous convergence}
 \lim_{n\to\infty} \frac{|\mathbb{S}^2|}{4\pi t_n} {L}_n^{t_n} f(y) =  \Delta_{\mathbb{S}^2}f(y)
\end{equation}
For the sake of the proof we will need also the following facts:
\begin{definition}{} (\cite{belkin2005towards})\\ \label{eq: my L^t} Let $\mu$ be the uniform probability measure on the manifold $\mathcal M$, and let $\text{vol}(\mathcal M)$ be the volume of $\mathcal M$. We define the functional approximation to the Laplace-Beltrami operator to be the operator $L^t: L^{2}(\mathcal{M}) \rightarrow L^{2}(\mathcal{M})$ such that
	\label{def:Functional approximation to the Laplace-Beltrami operator}
	$$ L^tf(y) = \int_{\mathcal M}\exp\left\{-\frac{||y-x||^2}{4t}\right\}\left(f(y)-f(x)\right)d\mu(x)$$
\end{definition}

\begin{prop} (\cite{belkin2005towards}) \\
Let $\mathcal M$ be a $k$-dimensional compact smooth manifold embedded in some Euclidean space $\mathbb R^N$, and fix $y\in\mathcal M$. Let the data points $x_1, \dots, x_n$ be sampled form a uniform distribution on the manifold $\mathcal M$. Set $t_n=n^{-\frac{1}{k+2+\alpha}}$, for any $\alpha>0$ and let $f\in\mathcal C_\infty(\mathcal M)$. Then
	$$\frac{1}{t}\frac{1}{(4\pi t)^{k/2}} L^tf(y) \xrightarrow{t\to 0 } \frac{1}{\text{vol}(\mathcal M)}\triangle_{\mathcal M}f(y).$$
	\label{prop:3}
\end{prop}

% brief introduction:
This proof consists in two steps: first, we prove that $\b L_n^t \rightarrow L^t$, and second, we use proposition \ref{prop:3} to prove equation \eqref{eq:continuous convergence}. This equation, as already explained in \cite{belkin2007convergence}, directly implies theorem \ref{theo:pointwise convergence for a regular sampling}.

Our first goal is thus to prove the following Proposition:
\vspace{0.5cm}
\begin{prop}\label{prop:1}
	For an equal area sampling $\{x_i\in\mathbb S^2\}_{i=0}^{n-1}: A_i=A_j \forall i,j$ of the sphere it is true that for all $f: \mathbb S^2 \rightarrow \mathbb R$ Lipschitz with respect to the Euclidean distance $||\cdot||$ with Lipschitz constant $\mathcal L_f$
	$$
	\left| \int_{\mathbb S^2}f({ x})\text{d}{\mu(x)} - \frac{1}{n}\sum_i f( x_i)\right|\leq \mathcal L_fd^{(n)}.
	$$
	Furthermore, for all $y\in\mathbb S^2$ the Heat Kernel Graph Laplacian operator $L^t_n$ converges pointwise to the functional approximation of the Laplace Beltrami operator $L^t$
	$$ L_n^tf(y)\xrightarrow{n\to\infty} L^tf(y).$$
\end{prop}
\vspace{0.5cm}

\begin{proof}

	Let us assume that the function $f:\mathbb R^3\rightarrow \mathbb R$ is Lipschitz with Lipschitz constant $\mathcal L_f$, we have

	$$\left| \int_{\sigma_{i}}f({ x})\text{d}{\mu(x)} - \frac{1}{n}f( x_i)\right| \leq \mathcal L_fd^{(n)}\frac{1}{n} $$

	So, by triangular inequality and by summing all the contributions of all the $n$ patches
	$$\left| \int_{\mathbb S^2}f({ x})\text{d}{\mu(x)} - \frac{1}{n}\sum_i f( x_i)\right| \leq \sum_i \left| \int_{\sigma_{i}}f({ x})\text{d}{\mu(x)} - \frac{1}{n}f( x_i)\right|\leq n  \mathcal L_fd^{(n)}\frac{1}{n} = \mathcal L_fd^{(n)}$$
	Thanks to this result, we have the following two pointwise convergences

	$$\forall f \text{ Lipschiz,}\quad \forall y\in\mathbb S^2,  \quad\quad \frac{1}{n}\sum_i e^{-\frac{||x_i-y||^2}{4t}}\rightarrow \int e^{-\frac{||x-y||^2}{4t}}d\mu(x)$$
	$$\forall f \text{ Lipschiz,}\quad \forall y\in\mathbb S^2,  \quad\quad \frac{1}{n}\sum_i e^{-\frac{||x_i-y||^2}{4t}}f(x_i)\rightarrow \int e^{-\frac{||x-y||^2}{4t}}f(x)d\mu(x)$$

	Definitions \ref{def:Heat Kernel Graph Laplacian operator} and \ref{def:Functional approximation to the Laplace-Beltrami operator} end the proof.
\end{proof}
\vspace{0.5cm}

Now, we just proved that \textit{keeping t fixed} $L_n^tf(x)\rightarrow L^tf(x)$. Now our goal is to prove that:

\vspace{0.5cm}
\begin{prop}\label{prop:2}
	Given a sampling regular enough, i.e., for which we assume $A_i=A_j \ \forall i,j\text{ and }d^{(n)}\leq \frac{C}{\sqrt{n}}$, for a fixed $t>0$, a fixed Lipschitz function $f$ and a fixed point $y\in\mathbb S^2$ there exists a sequence $t_n = n^\beta, \beta<0$ such that
$$
\forall f \text{ Lipschitz, } \forall x\in\mathbb S^2 \quad \left|\frac{1}{4\pi t_n^2}\left(L_n^{t_n}f(x) - L^{t_n}f(x)\right)\right|\xrightarrow{n\to \infty}0.
$$
\end{prop}
\vspace{0.5cm}

\begin{proof}[Proof of Proposition \ref{prop:2}]

We define for simplicity of notation
\begin{align}
	\phi^t(x;y) &:= e^{-\frac{||x-y||^2}{4t}}\left(f(y)-f(x)\right)\\
	K^t(x,y) &:=  e^{-\frac{||x-y||^2}{4t}}
\end{align}
We start by writing the following chain of inequalities
\begin{align*}
	||L_n^tf-L^tf||_\infty &= \max _{y\in \mathbb S^2} \left|L_n^tf(y)-L^tf(y)\right|\\
	&= \max _{y\in \mathbb S^2} \left| \frac{1}{n} \sum_{i=1}^n \phi^t(x_i; y)- \int_{\mathbb S^2} \phi^t(x;y)d\mu(x) \right|\\
	&\leq \max _{y\in \mathbb S^2}  \sum_{i=1}^n   \left| \frac{1}{n}  \phi^t(x_i; y)- \int_{\sigma_i} \phi^t(x;y)d\mu(x) \right|\\
	&\leq  \max _{y\in \mathbb S^2} \left[\mathcal L_{\phi^t_y}d^{(n)} \right]\\
\end{align*}
where $\mathcal L_{\phi^t_y}$ is the Lipschitz constant of $x \rightarrow \phi^t(x, y)$ and where we used for the last inequality Proposition \ref{prop:1}. If we assume $d^{(n)}\leq \frac{C}{\sqrt{n}}$ we have that

$$||L_n^tf-L^tf||_\infty  \leq  \max _{y\in \mathbb S^2} \left[ \mathcal L_{\phi^t_y} \frac{C}{\sqrt{n}} \right]$$

Let's now find the explicit dependence $t\rightarrow \mathcal L_{\phi^t_y}$
\begin{align*}
	\mathcal L_{\phi^t_y} &= ||\partial_x\phi^t(\cdot;y)||_\infty\\&
	= ||\partial_x\left(K^t(\cdot;y)f\right)||_\infty\\&
	= ||\partial_x K^t(\cdot;y)f + K^t(\cdot;y)\partial_x f||_\infty\\&
	\leq ||\partial_x K^t(\cdot;y)f||_\infty + ||K^t(\cdot;y)\partial_x f||_\infty\\&
	\leq  ||\partial_x K^t(\cdot;y)||_\infty||f||_\infty + ||K^t(\cdot;y)||_\infty||\partial_x f||_\infty\\&
	= ||\partial_x K^t(\cdot;y)||_\infty||f||_\infty + ||\partial_x f||_\infty\\&
	= \mathcal L_{K^t_y} ||f||_\infty + ||\partial_xf||_\infty\\&
	= \mathcal L_{K^t_y} ||f||_\infty + \mathcal L_f
\end{align*}
where $\mathcal L_{K^t_y}$ is the Lipschitz constant of $x\rightarrow K^t(x;y)$. We can observe that such constant does not depend on $y$:

$\mathcal L_{K^t_y} = \norm{\partial_x e^{-\frac{x^2}{4t}}}_\infty = \norm{\frac{x}{2t}e^{-\frac{x^2}{4t}}}_\infty = \left. \frac{x}{2t}e^{-\frac{x^2}{4t}}\right|_{x=\sqrt{2t}}=(2et)^{-\frac{1}{2}}\propto t ^ {-\frac{1}{2}}$

So we can continue
$$\begin{aligned}
	\max _{y\in \mathbb S^2} \left[  \mathcal L_{\phi^t_y} \frac{C}{\sqrt{n}} \right]
	&\leq  \frac{C}{\sqrt{n}} \left( (2et)^{-\frac{1}{2}} \norm{f}_\infty + \mathcal L_f \right)\\
	&\leq \frac{C\norm{f}_\infty}{\sqrt{n}(2et)^{1/2}} +   \frac{C}{\sqrt{n}}\mathcal L_f\\
\end{aligned}$$
So we have that, rescaling by a factor $\frac{1}{4\pi t^2}$
\begin{align*}
	\norm{\frac{1}{4\pi t^2}\left(L_n^tf-L^tf\right)}_\infty&\leq \frac{1}{4\pi t^2}\norm{\left(L_n^tf-L^tf\right)}_\infty \\
	&\leq \frac{C}{4\pi}\left[\frac{\norm{f}_\infty}{\sqrt{2e}}\frac{1}{\sqrt{n}t^{5/2}} + \frac{\mathcal L_f}{\sqrt{n}t^2}\right]
\end{align*}

we want $\begin{cases}
t \rightarrow 0\\
n \rightarrow \infty\\
\sqrt{n}t^{5/2} \rightarrow \infty\\
\sqrt{n}t^2 \rightarrow \infty
\end{cases}$ in order for $ \frac{C}{4\pi}\left[\frac{\norm{f}_\infty}{\sqrt{2e}}\frac{1}{\sqrt{n}t^{5/2}} + \frac{\mathcal L_f}{\sqrt{n}t^2}\right] \xrightarrow[t\to 0 ]{n\to\infty}0$

This is true if $\begin{cases}
t(n) = n^\beta, &\beta\in(-\frac{1}{5}, 0) \\
t(n) = n^\beta, &\beta\in(-\frac{1}{4}, 0)
\end{cases} \implies t(n) = n^\beta, \quad \beta\in(-\frac{1}{5}, 0)$

Indeed

$\sqrt{n}t^{5/2}=n^{5/2\beta+1/2}\xrightarrow{n \to \infty} \infty$ since $\frac{5}{2}\beta+1/2>0 \iff \beta>-\frac{1}{5}$

$\sqrt{n}t^2=n^{2\beta+1/2}\xrightarrow {N \to \infty} \infty$ since $2\beta+1/2>0 \iff \beta>-\frac{1}{4}$

So, for $t=n^\beta$ with $\beta\in(-\frac{1}{5}, 0)$ we have that

$$\begin{cases}
(t_n)\xrightarrow{n\to\infty}0\\
\norm{\frac{1}{4\pi t_n^2}L_n^{t_n}f-\frac{1}{4\pi t_n^2}L^{t_n}f}_\infty  \xrightarrow{n\to\infty}0
\end{cases}$$

\end{proof}


Theorem  \ref{theo:pointwise convergence for a regular sampling}, is then an immediate consequence of Proposition \ref{prop:2} and Proposition \ref{prop:3}.
\begin{proof}[Proof of Theorem \ref{theo:pointwise convergence for a regular sampling}]
	Thanks to Proposition \ref{prop:2} and Proposition \ref{prop:3}	we conclude that $\forall y\in\mathbb S^2 $
	$$\lim_{n\to\infty}\frac{1}{4\pi t_n^2} L_n^{t_n}f(y) =  \lim_{n\to\infty}\frac{1}{4\pi t_n^2} L^{t_n}f(y) = \frac{1}{|\mathbb S^2|}\triangle_{\mathbb S^2}f(y) $$
\end{proof}

The proof of this result is instructive since it shows that we need to impose some regularity conditions on the sampling. If the sampling is equal area as HEALPix, meaning that all the patches $\sigma_i$ have the same area, then we need to impose that $ d^{(n)}\leq \frac{1}{\sqrt{n}}$. If the sampling is not equal area, meaning that in general $A_i\neq A_j$, it can be shown that we need a slightly more complex condition: $\max_{i=0,\dots,n-1}d_iA_i\leq Cn^{-\frac{3}{2}}$.\\
In the work of Belkin et al. \cite{belkin2005towards} the sampling is drawn form a uniform random distribution on the sphere, and their proof heavily relies on the uniformity properties of the distribution from which the sampling is drawn. In our case the sampling is deterministic, and the fact that for a sphere there doesn't exist a regular sampling with more than 12 points (the vertices of a icosahedron) is indeed a problem that we need to overcome by imposing the regularity conditions above.

To conclude, we can see that the result obtained has the same form than the result obtained in \cite{belkin2005towards}. Given the kernel density $t(n)=n^\beta$, if Belkin et al. proved convergence in the random case for $\beta \in (-\frac{1}{4}, 0)$, we proved convergence in the HEALPix case for $\beta \in (-\frac{1}{5}, 0)$. This kind of result can be interpreted in the following way. In order to have this pointwise convergence, we need to reduce the kernel width but \textit{not so fast} compared to the resolution of the graph. In other words, the kernel width has to be reduced but is somewhat limited by the resolution of the graph. In the next section we'll see how to set in practice a good kernel width $t$ given a graph resolution $n$.
\begin{remark}
	Pointwise convergence is just a necessary condition for spectral convergence.  Theorem \ref{theo:pointwise convergence in the healpix case} does not imply convergence of eigenvalues and eigenvectors.
\end{remark}

\section{Experimental details}

* Network architecture and details

\subsection{3D object Recognition}
\subsubsection*{Detailed results}
\paragraph*{SHREC17}

\begin{table}
    \centering
    \begin{tabular}{l|c c r r}
        \multicolumn{1}{l}{} & \multicolumn{2}{c}{performance} & \multicolumn{2}{c}{speed}\\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5}
        \multicolumn{1}{l}{Method} & Accuracy & F1-score & inference & training \\ \hline
        Cohen \emph{s2cnn} & - & - & 38ms & 50h\\
        % Cohen \emph{s2cnn\_simple} & 78.59 & 78.85 & 400k & 12ms & 32h\\
        Esteves \emph{sphericalcnn} & 79.18 & 79.36 & 9.8ms & 2h52\\ \hline
        DeepSphere \emph{Equiangular} & 79.25 & 79.36 & 0.9ms & 47m \\
        DeepSphere \emph{HEALPix} & 80.42 & 80.65 & 0.9ms & 46m\\
        DeepSphere \emph{Improved HEALPix} & 80.76 & & &
    \end{tabular}
    \caption{Performance of different models}
    \label{tab:SHREC17_class}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{l|c c c c|c c c c}
     & \multicolumn{4}{c|}{micro (label average)} & \multicolumn{4}{c}{macro (instance average)} \\
    Method & P@N & R@N & F1@N & mAP & P@N & R@N & F1@N & mAP \\ \hline
    Cohen \emph{s2cnn} & 0.701 & 0.711 & 0.699 & 0.676 & - & - & - & - \\
    % Cohen \emph{s2cnn\_simple} & 0.704 & 0.701 & 0.696 & 0.665 & 0.430 & 0.480 & 0.429 & 0.385\\
    Esteves \emph{sphericalcnn} & 0.717 & 0.737 & - & 0.685 & 0.450 & 0.550 & - & 0.444\\ \hline
    DeepSphere \emph{Equiangular} & 0.709 & 0.700 & 0.698 & 0.665 & 0.439 & 0.489 & 0.439 & 0.403 \\
    DeepSphere \emph{HEALPix} & 0.725 & 0.717 & 0.715 & 0.686 & 0.475 & 0.508 & 0.468 & 0.428\\
    DeepSphere \emph{Improved HEALPix} & & & & &
    \end{tabular}
    \caption{Performance of different models over SHREC17 perturbed dataset as a retrieval task, using the official script of the competition.}
    \label{tab:SHREC17_retriev}
\end{table}

\paragraph*{ModelNet 40}

\begin{table}
    \centering
    \begin{tabular}{c|cccc}
         &  x/x & z/z & SO3/SO3 & z/SO3 \\ \hline
Cohen & 85.0 & - & - & - \\
Jiang & 90.5 & - & - & - \\
Esteves \emph{scnn} & - & 88.9 & 86.9 & 78.6 \\
%Esteves \emph{MVCNN} & 94.69 & - & - & - \\
DeepSphere & 87.8 & 86.8 & 86.7 & 76.9
    \end{tabular}
    \caption{Accuracy (in percent) on test set, for different models. The x dataset has no augmentation, the z dataset is augmented with rotation around Z-axis, and SO3 with ZYZ rotations.}
    \label{tab:mn40_results}
\end{table}

\subsubsection*{HEALPix}

HEALPix sampling, input signal at Nside = 32

CNN having 5 convolutional layers, a global average pooling layer and a fully-connected layer at the end, such as this:
\begin{dmath}
    [GC_{16}\, +\, BN\, +\, ReLU]_{nside32}\, +\, \textrm{Pool}\, +\, [GC_{32}\, +\, BN\, +\, ReLU]_{nside16}\, +\, \textrm{Pool}\, +\, [GC_{64}\, +\, BN\, +\, ReLU]_{nside8}\, +\, \textrm{Pool}\, +\, [GC_{128}\, +\, BN\, +\, ReLU]_{nside4}\, +\,\textrm{Pool}\, +\, [GC_{256}\, +\, BN\, +\, ReLU]_{nside2}\, +\, \textrm{Pool}\, +\, GAP\, +\, FCN\, +\, \textrm{softmax}
\end{dmath}
Each convolutional layer has a graph convolution using Chebyshev polynomial approximation, batch normalization and ReLU activation. The order of the Chebyshev polynomial is 4 for each layer, the number of feature maps are [16, 32, 64, 128, 256] respectively and down-sampling to the immediate smaller resolution for each layer.

number of parameter: 190k TODO exact number?

batch size 32, adam optimizer, constant learning rate of $5 \cdot 10^{-2}$
cross-entropy loss, and add triplet loss from ``Esteves et al.''

30 epochs with augmented dataset, 3 random translation perturbation, or 90 epochs over dataset that can be augmented
\subsubsection*{Equiangular}
Equiangular sampling with bandwidth of 64 in longitude and latitude.
The architecture of the Neural Network is exactly the same, with a stride of [2, 2] for each pooling layer
\subsection{cosmo}
HEALPix sampling, Nside = 1024, with improved graph

\begin{dmath}
    [GC_{16}\, +\, BN\, +\, ReLU]_{nside1024}\, +\, \textrm{Pool}\, +\, [GC_{32}\, +\, BN\, +\, ReLU]_{nside512}\, +\, \textrm{Pool}\, +\, [GC_{64}\, +\, BN\, +\, ReLU]_{nside256}\, +\, \textrm{Pool}\, +\, [GC_{64}\, +\, BN\, +\, ReLU]_{nside128}\, +\,\textrm{Pool}\, +\, [GC_{64}\, +\, BN\, +\, ReLU]_{nside64}\, +\, \textrm{Pool}\, +\, [GC_{2}]_{nside32}\, +\, GAP\, +\, \textrm{softmax}
\end{dmath}

The order of the Chebyshev polynomial is 5 for each layer, the number of feature maps are [16, 32, 64, 64, 64, 3] respectively and down-sampling to the immediate smaller resolution for each layer.

batch size = 8, 80 epochs, adam optimizer, cross-entropy loss, learning rate $2\cdot10^{-4}\cdot0.999^{\textrm{step}}$

\subsection{climate event detection}
\subsubsection*{Results}

* DS-jiang is the similar architecture as \cite{jiang2019sphericalcnn}. \\
* DS-optimal has 2 times the number of feature maps of DS. \\
* \todo{What is DS and DS-Cohen?} \\
* \todo{Is (equi non-weighted) the full resolution?} \\
* \todo{check column order for DS} \\

\begin{table}
    \centering
	\begin{tabular}{l l l l l}
		\toprule
        & TC & AR & BG & mean \\
		\midrule
		\cite{mudigonda2017climateevents} & 74 & 65 & 97 & 78.67 \\
		\cite{jiang2019sphericalcnn} (paper) & 94 & 93 & 97 & 94.67 \\
		\cite{jiang2019sphericalcnn} (rerun) & 93.9 & 95.7 & 95.2 & 94.95 \\
        \cite{cohen2019gauge} (S2R) & 97.8 & 97.3 & 97.3 & 97.5 \\
        \cite{cohen2019gauge} (R2R) & 97.9 & 97.8 & 97.4 & 97.7 \\
		\midrule
        DS (weighted) & $97.4\pm 1.1$ & $97.7\pm 0.7$ & $98.2\pm 0.5$ & $97.8\pm 0.3$ \\
        DS-Jiang (weighted) & 97.1 & 97.6 & 96.5 & 97.1 \\
        DS-optimal (weighted) & 91.5 & 93.4 & 99.0 & 94.6 \\
		\midrule
        DS (non-weighted) & $69.2\pm 3.7$ & $94.5\pm 2.9$ & $99.7\pm 0.1$ & $87.8\pm 0.5$ \\
        DS-Jiang (non-weighted) & 33.6 & 93.6 & 99.3 & 75.5 \\
        DS-optimal (non-weighted) & 73.4 & 92.7 & 99.8 & 88.7 \\
		\midrule
		\todo{DS-Cohen (weighted)} & 95.5 & 96.9 & 93.9 & 95.4 \\
		\todo{DS-Cohen (non-weighted)} & 95.5 & 96.9 & 93.9 & 95.4 \\
		\todo{DeepSphere (equi non-weighted)} & 31.3 & 75.2 & 99.9 & 68.80 \\
		\bottomrule
    \end{tabular}
    \caption{
		Results on climate event segmentation: accuracy.
		Tropical cyclones (TC) and atmospheric rivers (AR) are the two positive classes, against the background (BG).
		\todo{\citet{mudigonda2017climateevents} is not directly comparable as they don't use the same input feature maps.}
		Note that a non-weighted cross-entropy loss is not optimal for the accuracy metric.
	}
\end{table}

\begin{table}
	\centering
	\begin{tabular}{l l l l}
		\toprule
        & TC & AR & mean \\
		\midrule
		\cite{jiang2019sphericalcnn} (rerun) & 11.08 & 65.21 & 38.41 \\
        \cite{cohen2019gauge} (S2R) & - & -& 68.6 \\
        \cite{cohen2019gauge} (R2R) & - & -& 75.9 \\
		\midrule
        DS (non-weighted) & $80.86\pm 2.42$ & $97.45\pm 0.38$ & $89.16\pm 1.37$ \\
        DS-Jiang (non-weighted) & 46.2 & 93.9 & 70.0 \\
        DS-optimal (non-weighted) & 84.71 & 98.05 & 91.38 \\
		\midrule
        DS (weighted) & $58.88\pm 3.17$ & $95.41\pm 1.51$ & $77.15\pm 1.94$ \\
        DS-Jiang (weighted) & 49.7 & 89.2 & 69.5 \\
        DS-optimal (weighted) & 52.80 & 94.78 & 73.79 \\
		\midrule
		\todo{DS-Cohen (weighted)} & 13.1 & 80.3 & 46.7 \\
		\todo{DeepSphere (equi non-weighted)} & 55.53 & 94.85 & 75.19 \\
		\bottomrule
    \end{tabular}
    \caption{
		Results on climate event segmentation: average precision.
		Tropical cyclones (TC) and atmospheric rivers (AR) are the two positive classes.
		Note that a weighted cross-entropy loss is not optimal for the average precision metric.
	}
\end{table}

\begin{table}
	\centering
	\begin{tabular}{l r r r}
		\toprule
		& \multicolumn{1}{c}{size} & \multicolumn{2}{c}{speed} \\
        \cmidrule(lr){2-2} \cmidrule(lr){3-4}
		& params & inference & training \\
		\midrule
        %Mudigonda et al. & 97 & 74 & 65 & 78.67 & - & - & - \\
		\cite{jiang2019sphericalcnn} & 330\,k & 10\,ms & 10\,h \\ % 328'339
		% \cite{cohen2019gauge} (S2R) & - & - & - \\
        % \cite{cohen2019gauge} (R2R) & - & - & - \\
		DeepSphere (Jiang) & 590\,k & 5\,ms & 3\,h \\ % 590k
        DeepSphere & 13\,M & 33\,ms & 13\,h \\ % 12'926'432
		DeepSphere (optimal) & 52\,M & 50\,ms & 20\,h \\
		\todo{DS-Cohen (weighted)} & 2\,M & 11\,ms & 4\,h \\
		\todo{DeepSphere (equi non-weighted)} & - & 567\,ms & 48\,h \\
		\bottomrule
    \end{tabular}
    \caption{
		Results on climate event segmentation: size and speed.
	}
\end{table}

\subsubsection*{Icosahedron}

icosahedron sampling, level-5 resolution

CNN with encoder decoder architecture
encoder:\\
\begin{dmath}
    [GC_{32}\, +\, BN\, +\, ReLU]_{L5}\,+\, [GC_{64}\, +\, BN\, +\, ReLU]_{L5}\, +\, \textrm{Pool}\, +\, [GC_{128}\, +\, BN\, +\, ReLU]_{L4}\, +\, \textrm{Pool}\, +\, [GC_{256}\, +\, BN\, +\, ReLU]_{L3}\, +\,\textrm{Pool}\, +\, [GC_{512}\, +\, BN\, +\, ReLU]_{L2} +\,\textrm{Pool}\, +\, [GC_{512}\, +\, BN\, +\, ReLU]_{L1} +\,\textrm{Pool}\, +\, [GC_{512}]_{L0}
\end{dmath}
decoder:\\
\begin{dmath}
    \textrm{Unpool}\, +\,[GC_{512}\, +\, BN\, +\, ReLU]_{L1}\, +\, \textrm{concat}\, +\, [GC_{512}\, +\, BN\, +\, ReLU]_{L1}\, +\, \textrm{Unpool}\, +\, [GC_{256}\, +\, BN\, +\, ReLU]_{L2}\, +\, \textrm{concat}\, +\, [GC_{256}\, +\, BN\, +\, ReLU]_{L2}\, +\, \textrm{Unpool}\, +\, [GC_{128}\, +\, BN\, +\, ReLU]_{L3}\, +\, \textrm{concat}\, +\, [GC_{128}\, +\, BN\, +\, ReLU]_{L3}\, +\,\textrm{Unpool}\, +\, [GC_{64}\, +\, BN\, +\, ReLU]_{L4}\,+\, \textrm{concat}\, +\, [GC_{64}\, +\, BN\, +\, ReLU]_{L4}\, +\,\textrm{Unpool}\,  +\, [GC_{32}\, +\, BN\, +\, ReLU]_{L5}\,+ \, [GC_3]_{L5}
\end{dmath}
% number of parameters: 2,2M

concat the results of the corresponding encoder layer before the graph convolution. Not similar to Jiang and 4 times the feature maps.

batch size 64, adam optimizer, constant learning rate of $1 \cdot 10{-3}$, cross-entropy loss, both weighted and non weighted. Weight chosen with scikit-learn "compute\_class\_weight" on the training set

30 epochs
\subsubsection*{DS-jiang}
encoder:\\
\begin{dmath}
    [GC_{8}\, +\, BN\, +\, ReLU]_{L5}\,+\,\textrm{Pool}\,+\, [GC_{16}\, +\, BN\, +\, ReLU]_{L4}\, +\, \textrm{Pool}\, +\, [GC_{32}\, +\, BN\, +\, ReLU]_{L3}\, +\, \textrm{Pool}\, +\, [GC_{64}\, +\, BN\, +\, ReLU]_{L2}\, +\,\textrm{Pool}\, +\, [GC_{128}\, +\, BN\, +\, ReLU]_{L1}\, +\, \textrm{Pool}\,  +\, [GC_{128}\, +\, BN\, +\, ReLU]_{L0}
\end{dmath}
decoder:\\
\begin{dmath}
    \textrm{Unpool}\, +\,[GC_{128}\, +\, BN\, +\, ReLU]_{L1}\, +\, \textrm{concat}\, +\, [GC_{128}\, +\, BN\, +\, ReLU]_{L1}\, +\, \textrm{Unpool}\, +\, [GC_{64}\, +\, BN\, +\, ReLU]_{L2}\, +\, \textrm{concat}\, +\, [GC_{64}\, +\, BN\, +\, ReLU]_{L2}\, +\, \textrm{Unpool}\, +\, [GC_{32}\, +\, BN\, +\, ReLU]_{L3}\, +\, \textrm{concat}\, +\, [GC_{32}\, +\, BN\, +\, ReLU]_{L3}\, +\,\textrm{Unpool}\, +\, [GC_{16}\, +\, BN\, +\, ReLU]_{L4}\,+\, \textrm{concat}\, +\, [GC_{16}\, +\, BN\, +\, ReLU]_{L4}\, +\,\textrm{Unpool}\,  +\, [GC_{8}\, +\, BN\, +\, ReLU]_{L5}\,+\,\textrm{concat}\, +\, [GC_{8}\, +\, BN\, +\, ReLU]_{L5}\, + \,[GC_3]_{L5}
\end{dmath}
% number of parameters: 590'976  % more feat is 16 times more

concat the results of the corresponding encoder layer after the graph convolution. Similar to ``Jiang et al.''
\subsubsection*{Equiangular}
Equiangular sampling, with a latitude bandwidth 384 and longitude bandwidth 576.

Due to memory problem, could not use the same architecture

8 epochs

encoder:\\
\begin{dmath}
    [GC_{16}\, +\, BN\, +\, ReLU]_{bw}\,+\, [GC_{16}\, +\, BN\, +\, ReLU]_{bw}\, +\, \textrm{Pool}\, +\, [GC_{32}\, +\, BN\, +\, ReLU]_{bw/16}\, +\, \textrm{Pool}\, +\, [GC_{64}\, +\, BN\, +\, ReLU]_{bw/64}\, +\, [GC_{128}\, +\, BN\, +\, ReLU]_{bw/64}
\end{dmath}
decoder:\\
\begin{dmath}
    \textrm{Unpool}\, +\,[GC_{128}\, +\, BN\, +\, ReLU]_{bw/64}\, +\, \textrm{concat}\, +\, [GC_{128}\, +\, BN\, +\, ReLU]_{bw/64}\, +\, \textrm{Unpool}\, +\, [GC_{64}\, +\, BN\, +\, ReLU]_{bw/64}\, +\, \textrm{concat}\, +\, [GC_{64}\, +\, BN\, +\, ReLU]_{bw/64}\, +\, \textrm{Unpool}\, +\, [GC_{32}\, +\, BN\, +\, ReLU]_{bw/16}\, +\, \textrm{concat}\, +\, [GC_{32}\, +\, BN\, +\, ReLU]_{bw/16}\, +\,\textrm{Unpool}\, +\, [GC_{16}\, +\, BN\, +\, ReLU]_{bw}\,+\, \textrm{concat}\, +\, [GC_{16}\, +\, BN\, +\, ReLU]_{bw}\, +\,\textrm{Unpool}\,  +\, [GC_{16}\, +\, BN\, +\, ReLU]_{bw}\,+\,[GC_3]_{bw}
\end{dmath}

batch size 1
\subsection{GHCN}
non-regular sampling, thus no pooling was used

batch size 64, RMSprop optimizer, constant learning rate of $1 \cdot 10{-3}$, MSE loss, order of 5 for all layers

250 epochs with no augmentation

\subsubsection*{Results}

\begin{table}
    \centering
    \begin{tabular}{c|ccc}
        order & MSE & MAE & R2 \\ \hline
        0 & 10.88 & 2.42 & 0.896\\
        1 & 8.91 & 2.20 & 0.906\\
        4 & 8.20 & 2.11 & 0.919\\
        9 & 8.38 & 2.12 & 0.915\\
    \end{tabular}
    \caption{Future temperature estimation task. Evolution of performance in respect of the order of the graph filter}
    \label{tab:future_results}
    \hfill
    %~\\[1cm]
    % \begin{minipage}{0.48\linewidth}
    % \begin{tabular}{c|ccc}
    %     order & MSE & MAE & R2 \\ \hline
    %     1 & 134.01 & 10.51 & -0.319\\
    %     4 & 129.48 & 10.31 & -0.274\\
    %     9 & 141.68 & 10.82 & -0.389\\
    % \end{tabular}
    % \caption{Baseline: testing with the present day (T-1) instead of the future day T}
    % \label{tab:present_results}
    % \end{minipage}
\end{table}

\begin{table}
    \centering
    \begin{minipage}{0.48\linewidth}
    \begin{tabular}{c|ccc}
        order & MSE & MAE & R2 \\ \hline
        0 & 0.58 & 0.42 & -0.92\\
        4 & 0.50 & 0.18 & 0.597\\
    \end{tabular}
    \caption{Find day in year using precipitation}
    \label{tab:glob_prec}
    \end{minipage}
    \hfill
    %~\\[1cm]
    \begin{minipage}{0.48\linewidth}
    \begin{tabular}{c|ccc}
        order & MSE & MAE & R2 \\ \hline
        0 & 0.10 & 0.10 & 0.881\\
        4 & 0.05 & 0.05 & 0.969\\
    \end{tabular}
    \caption{Using temperature feature that is already periodical}
    \label{tab:glob_all}
    \end{minipage}
\end{table}

\subsubsection*{Dense}

\begin{dmath}
    [GC_{50}\, +\, BN\, +\, ReLU]\, +\, [GC_{100}\, +\, BN\, +\, ReLU]\, +\, [GC_{100}\, +\, BN\, +\, ReLU]\, +\, [GC_{1}\, +\, BN\, +\, ReLU]
\end{dmath}

\subsubsection*{Global}

\begin{dmath}
    [GC_{50}\, +\, BN\, +\, ReLU]\, +\, [GC_{100}\, +\, BN\, +\, ReLU]\, +\, [GC_{100}\, +\, BN\, +\, ReLU]\, +\, GAP\, +\, FCN
\end{dmath}



\end{document}
